\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{tikz}
\usepackage{listings}
\usepackage{mathtools}


\usetikzlibrary{trees}
\usetheme{CambridgeUS}
\usecolortheme{crane}

\title{From Abstraction to Computation: \\ Understanding Lambda Calculus}
\author{Pratham Gupta  Gavish Bansal\\
        Sehaj Ganjoo  Krishna Agarwal}
\institute{Indian Institute of Science, Bengaluru}
\date{\today}

\AtBeginSection[]
{
  \begin{frame}[allowframebreaks]
    \frametitle{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}

% \AtBeginSubsection[]
% {
%   \begin{frame}[allowframebreaks]
%     \frametitle{Outline}
%     \tableofcontents[currentsection,currentsubsection]
%   \end{frame}
% }


\begin{document}

% Slide 1: Title Slide
\begin{frame}
  \titlepage
\end{frame}

% Slide 2: Outline
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% Section 1: Introduction and Motivation
\section{Introduction and Motivation}

\begin{frame}{Historical Background}
\textbf{Leibniz’s Ideal:}
\begin{itemize}
    \item A "universal language" to express all possible problems.
    \item A decision method to solve all problems in this language.
\end{itemize}
\bigskip
By the early 20th century, set theory and first-order logic (Frege, Russell, Zermelo) fulfilled point (1).\\
However, point (2) remained open—this became the Entscheidungsproblem ("decision problem"):\\
\begin{quote}
    Can all problems be solved mechanically?
\end{quote}
\end{frame}
\begin{frame}{Historical Background}
Alonzo Church and Alan Turing independently proved that no general algorithm can decide the truth of all mathematical statements.\\
In order to do so, they had to formalize "computability.\\
\begin{itemize}
    \item Church (1936): Introduced lambda calculus as a formal model of computation
    \item Turing (1936/37): Introduced Turing machines as an alternative model.
    \item Turing (1937): Proved both models are equivalent—defining the same class of computable functions
\end{itemize}

\end{frame}
%Hilbert’s Entscheidungsproblem (1928):
% (“decision problem”)
% Given a sentence in first-order logic, give an “effectively calculable procedure” for determining if it’s provable.
% Mathematicians: “we should probably try to formalize what counts as an ‘algorithm’ ”.
% \begin{frame}{Historical Background}
%   \begin{itemize}
%     \item Developed by Alonzo Church in the 1930s.
%     \item Originally intended as a foundation for mathematics.
%     \item Inspired by higher-order logic and the concept of functions.
%     \item Later became a model of computation equivalent to Turing machines.
%   \end{itemize}
% \end{frame}

\begin{frame}{Motivation}
  \begin{itemize}
    \item Addressing Russell’s paradox in set theory.
    \item Establishing a formal system for computability.
    \item Laying the groundwork for functional programming languages.
  \end{itemize}
\end{frame}

\begin{frame}{Church-Turing Thesis}
Any natural / reasonable notion of computation realizable in the physical world can be simulated by a TM (or equivalently, by lambda calculus)
\end{frame}

\begin{frame}{Overview of Topics}
  \begin{itemize}
    \item Syntax of the lambda-calculus.
    \item Free and bound variables.
    \item Substitution and $\alpha$-conversion.
    \item $\beta$-reduction and the Church–Rosser theorem.
    \item Combinators and representation of booleans.
    \item Encoding natural numbers.
    \item Fixed-point combinators and recursion.
    \item $\lambda$-definability of computable functions.
  \end{itemize}
\end{frame}

% Section 2: Syntax of the Lambda Calculus
\section{Syntax of the Lambda Calculus}
\begin{frame}{Basic Concepts}
  \begin{itemize}
    \item \textbf{Variables:} $x, y, z, \dots$
    \item \textbf{Abstraction:} $\lambda x.M$ (function definition)
    \item \textbf{Application:} $(MN)$ (function application)
  \end{itemize}
\end{frame}

\subsection{$\lambda$-Terms}
\begin{frame}{Formal Definition of $\lambda$-Terms}
  \begin{block}{Definition}
    The set of $\lambda$-terms is defined inductively:
    \begin{enumerate}
      \item Any variable $x$ is a $\lambda$-term.
      \item If $M$ and $N$ are $\lambda$-terms, then $(M N)$ is a $\lambda$-term called an \textbf{application}
      \item If $M$ is a $\lambda$-term and $x$ is a variable, then $\lambda x.M$ is a $\lambda$-term called a $\lambda$-abstraction
    \end{enumerate}
  \end{block}
  % \textbf{Examples:}
  % \begin{itemize}
  %     \item Let \(M = \lambda x.x\), which is a lambda abstraction.
  %     \item Let \(N = y\).
  %     \item \((M N) = (\lambda x.x \; y) \rightarrow y\).
  % \end{itemize}
\end{frame}

\begin{frame}{Formal Definition of $\lambda$-Terms}
  \begin{block}{Some Valid Lambda Expressions:}
  \begin{itemize}
      \item $x$
      \item $\lambda x.\lambda x.\,x$ 
      \item $xy$
      \item $x\lambda y.(x(yy))$
      \item $\lambda x.\lambda y.\lambda z.\,(x\;(y\;z))$
      \item $\lambda\lambda x.x$ (invalid)
  \end{itemize}
  \end{block}
\end{frame}

% \begin{frame}{Tree Representation}
%     \begin{itemize}
%       \item Each $\lambda$-term can be represented as a labeled tree.
%       \item Leaves represent variables.
%       \item Interior nodes represent applications or abstractions.
%     \end{itemize}
%     \begin{center}
%       % Replace with your own image if desired
%         \includegraphics[width=0.5\textwidth]{images/treeimage.png}
%     \end{center}
% \end{frame}


\begin{frame}{Notation Conventions}
  \begin{itemize}
    \item Left-association of application:
      \[
        (((F \; M_1)M_2)\ldots M_n) \quad\text{is written as}\quad F M_1 \ldots M_n.
      \]
      e.g. \(wxyz\) is \(((wx)y)z\).
    \item Right-association of abstractions:
      \[
        \lambda x_1.\lambda x_2.\cdots\lambda x_n.M \quad\text{is written as}\quad \lambda x_1 \cdots x_n.M.
      \]
      e.g. \(\lambda x.xy\) is \(\lambda x.(xy)\), and
      \(\lambda x.\lambda x.x\) is \(\lambda xx.x\).
  \end{itemize}
\end{frame}
\begin{frame}{Semantics}
  \textbf{$\lambda$x.M} defines a function, where:
  \begin{itemize}
    \item $x$ is the formal parameter of the function.
    \item $M$ is the body of the function.
    \item $M \to M_1 M_2$, function application, similar to calling function $M_1$ and setting its formal parameter to $M_2$.
  \end{itemize}
  \vspace{1em}
  \textbf{Example:}
  \begin{itemize}
    \item $\lambda x.\,(x+1)$ defines a function that adds 1 to its argument.
    \item ($\lambda x.\, (x+1$)) 2 evaluates as $(2+1)$, which is 3.
  \end{itemize}
  \begin{block}{}
    Q. How can + function be defined if abstractions only accept 1 parameter?\\
  \end{block}
\end{frame}

\subsection{Currying}
\begin{frame}
  \frametitle{Currying}
  \begin{block}{Definition}
    In lambda calculus, a abstraction takes only one argument.\\
    \textbf{Currying}( named after Haskell Curry) is the process of transforming a function that takes multiple arguments into a sequence of functions, each taking a single argument.
  \end{block}
  Example:
  \[
  \lambda x.\lambda y.(x+y)
  \]
  \[
  (\lambda x.\lambda y.(x+y))\ 10\ 20 \rightarrow (\lambda y.(10+y))\ 20 \rightarrow (10+20) \rightarrow 30
  \]
\end{frame}

\begin{frame}{Currying}

  Consider the function: \(f:\mathbb{N}\times\mathbb{N} \rightarrow \mathbb{N}\) \\
  If we fix the first argument, we get a function \(F_x:\mathbb{N} \rightarrow \mathbb{N}\)\\
  \[
    F_x(y) = f(x,y) \forall y \in \mathbb{N}
  \]  
  So we can write \(F_x = \lambda y.f(x,y)\)
  and then the function \(x \mapsto F_x\) can be written as:
  \[
    F = \lambda x.F_x = \lambda x.\lambda y.f(x,y)
  \]
  Hence we obtain 
  \[
    (F\;M)N \rightarrow_\beta F_M N \rightarrow_\beta f(M,N)
  \]
\end{frame}
\begin{frame}
  \frametitle{Currying}

  Hence we have ability to transform a function that takes multiple arguments into a sequence of functions, each taking a single argument.\\
  As a notational convention, we can write:
  \[
  \begin{aligned}
      F &= \lambda x_1.\lambda x_2.\cdots\lambda x_n.f(x_1,x_2,\ldots,x_n) \\ 
        &= \lambda x_1 x_2 \cdots x_n. f(x_1,x_2,\ldots,x_n)
  \end{aligned}
  \]
\end{frame}

\subsection{Free and Bound Variables}
\begin{frame}{Free Variables}
  Intuitively\\
  Free Variables(FV): Variables that are not bound by any abstraction.
  \begin{itemize}
    \item Is $x$ free in $\lambda x.x$? (No)
    \item Is $x$ free in $(\lambda x.xy)x$? (Yes)\\
    $(\lambda x.xy)x \to (xy)$
  \end{itemize}
  \begin{block}{Definition}
    For any $\lambda$-term \(M\), the set of free variables is denoted as \(\text{FV}(M)\) and is defined as:
    \begin{itemize}
      \item If \(M = x\) then: \text{FV}(x) = \{x\}
      \item If \(M = (M_1 M_2)\), then:$\text{FV}(M) = \text{FV}(M_1) \cup \text{FV}(M_2)$,
      \item If \(M = \lambda x.M_1\), then:$\text{FV}(M) = \text{FV}(M_1) \setminus \{x\}$
    \end{itemize}

  \end{block}
\end{frame}
\begin{frame}{Bound Variables}
  Intuitively\\
  Bound Variables(BV): Variables that are not free.\\
  Bound variables are declared within a $\lambda$-abstraction.\\
  \begin{itemize}
    \item $(\lambda \textcircled{x}.\lambda y.\textcircled{x}y) (\lambda z.\mathbf{x}z)$
  \end{itemize}
  \begin{block}{Definition}
    For any $\lambda$-term \(M\), the set of bound variables is denoted as \(\text{BV}(M)\) and is defined as:
    \begin{itemize}
      \item If \(M = x\) then: $\text{BV}(x) = \emptyset$
      \item If \(M = (M_1 M_2)\), then:$\text{BV}(M) = \text{BV}(M_1) \cup \text{BV}(M_2)$,
      \item If \(M = \lambda x.M_1\), then:$\text{BV}(M) = \text{BV}(M_1) \cup \{x\}$
    \end{itemize}
  \end{block}
\end{frame}
\begin{frame}{Combinators}

  \begin{definition}
    A $\lambda$-term M is closed or a \textbf{Combinator} if it has no free variables, i.e., $\text{FV}(M) = \emptyset$.
  \end{definition}
  \vspace{1em}
  \[
  \text{For } M_1 = \lambda x. (xy), \quad \text{FV}(M_1)=\{y\},\quad \text{BV}(M_1)=\{x\}.
  \]
  \[
  \text{For } M_2 = \lambda x.(\lambda y. (x)), \quad \text{FV}(M_2)=\emptyset,\quad \text{BV}(M_2)=\{x,y\}.
  \]
  Note that:
  \begin{itemize}
    \item \(M_2\) has no free variables and is a combinator.
    \item If we rename a bound variable in a term, It has no effect on the behavior of the term.
  \end{itemize}
\end{frame}



% Section 4: Substitution and Alpha-Conversion
\section{Substitution and $\alpha$-Conversion}
\begin{frame}{$\alpha$ Equivalence}
  \begin{block}{\strut}
  Q. What does it mean for two functions to be equivalent?
  \end{block}
  - Is $\lambda x.\mathtt{xy} \equiv \lambda y.\mathtt{yx}$?
  \begin{itemize}
    \item $\alpha$-equivalence is when two functions vary only by the names of bound variables.
    \item $M_1 \equiv_{\alpha} M_2$ if $M_1$ can be obtained from $M_2$ by renaming bound variables.
  \end{itemize}
\end{frame}
% \begin{frame}{Renaming ($\alpha$ conversion)}
%   $M\{y/x\}$: Rename bound variable $x$ to $y$ in $M$.\\
%   \vspace{1em}
%   \begin{itemize}
%     \item $x\{y/x\} = y$ and $z\{y/x\} = z$ if $z \neq x$.
%     \item $(M_1 M_2)\{y/x\} = (M_1\{y/x\})(M_2\{y/x\})$
%     \item $(\lambda x.M)\{y/x\} = (\lambda y.M\{y/x\})$
%     \item $(\lambda z.M)\{y/x\} = (\lambda z.M\{y/x\})$ if $x \neq z$.
%   \end{itemize}
% \end{frame}


% \begin{frame}{$\alpha$-Equivalence}
%   Example:\\
%   \[
%   (\lambda x. x \lambda y. xyz) \{w/x\} \longrightarrow
%   (\lambda x. x) \{w/x\} (\lambda y. xyz) \{w/y\} \longrightarrow
%   (\lambda y. y) (\lambda y. wyz)
%   \]
%   \begin{itemize}
%     \item Renaming bound variables to avoid clashes.
%     \item \(\lambda x.M \equiv_\alpha \lambda y.M\{y/x\}\) provided \(y \notin FV(M)\).
%     \item Essential for correct substitution.
%   \end{itemize}
% \end{frame}

\begin{frame}{Substitution}
  Our goal: Reduce expressions by replacing variables with terms.\\
  e.g ($\lambda x.x$) 2 $\to$ 2\\
  \vspace{1em}
  \begin{block}{}
  Solution : Substitution Operation
  \end{block}
  \begin{itemize}
    \item Notation: $M[x := N]$.
    \item Replacing all free occurrences of a variable $x$ in $M$ by a term $N$.
  \end{itemize}
 % Adjust spacing to avoid overfull box
  Example:
  \begin{itemize}
    \item  $(x+1)[x:= 2]$ = (2+1)
    \item $(\lambda x.(x+1))[x:= \lambda y.z]$ = $(\lambda x.(x+1))$ (no free $x$)\\
    \item $(\lambda x.(xt))[t:= \lambda y.z]$ = $(\lambda x.(x \lambda y.z))$ \\
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Substitution}

  \begin{definition}
    A substitution $\varphi = [x_1 := N_1, \ldots, x_n := N_n]$ is a finite set of pairs where each \(x_i\) is a variable and \(N_i\) is a $\lambda$-term.\\
  \end{definition}
  \begin{definition}
    Given a substitution \(\varphi\) and any variable \(x_i\), \(\varphi_{-x_i}\) is a new substitution obtained by removing the pair \((x_i, N_i)\) from \(\varphi\).\\
  \end{definition}

\end{frame}

\begin{frame}{Substitution}
  \frametitle{Substitution Rules}
  
  Given any $\lambda$-term \(M\) and a substitution \(\varphi\), the result of applying \(\varphi\) to \(M\) is denoted by \(M[\varphi]\) and is defined as follows:
  \begin{enumerate}
    \item If \(M\) is a variable \(x\), then:
      \[
      M[\varphi] = 
      \begin{cases} 
        N_i & \text{if } x = x_i \text{ for some } i, \\
        x & \text{otherwise.}
      \end{cases}
      \]
    \item If \(M\) is of the form \((M \; N)\), then:
      \[
      M[\varphi] = (M[\varphi] \; N[\varphi])
      \]
    \item If \(M\) is of the form \(\lambda y.M_1\), then:
      \[
      M[\varphi] = 
      \begin{cases} 
        \lambda y.M_1[\varphi] & \text{if } y \notin \{x_i\}, \\
        \lambda y.M_1[\varphi_{-y}] & \text{if } y = x_i \text{ for some } i.
      \end{cases}
      \]
  \end{enumerate}
\end{frame}


\begin{frame}{Capturing}
  \begin{block}{Example}
    $(\lambda x.yx)[y := \lambda z.xz]$
  \end{block}
  \begin{itemize}
    \item Result ? : $(\lambda x.(\lambda z.\textcircled{x}z)x)$
    \item $x \in FV(\lambda z.xz)$
    \item $x \in BV(\lambda \textcircled{x}.(\lambda z.\textcircled{x}z)x)$
  \end{itemize}
  This is called Variable Capture.\\
  \vspace{1em}
  Capturing occurs when a free variable unintentionally becomes bound due to renaming or substitution, altering the meaning of an expression.
  \textbf{Problem:} Function's behavior is altered.
\end{frame}
\begin{frame}{$\alpha$-Conversion}
  \begin{block}{}
    \textbf{Solution:} Use $\alpha$-conversion to rename bound variables before substitution.
  \end{block}

    \[
      (\lambda x.\, yx)[y := \lambda z.\, xz]
    \]
    Direct substitution causes variable capture, so we first apply $\alpha$-conversion:
    \[
      \lambda x.\, yx \rightarrow_{\alpha} \lambda w.\, yw
    \]
    Now perform the substitution:
    \[
      (\lambda w.\, yw)[y := \lambda z.\, xz] = \lambda w.\, (\lambda z.\, xz)w
    \]
\end{frame}
\begin{frame}{Definition of $\alpha$-Conversion}
  \textbf{Immediate Alpha-Conversion} ($\rightarrow_{\alpha}$) allows renaming bound variables in lambda terms while preserving meaning.
  
  \vspace{0.5cm}
  \textbf{Rules:}
  \begin{itemize}
      \item $\lambda x. M \rightarrow_{\alpha} \lambda y. M[x := y]$, if $y \notin FV(M) \cup BV(M)$.
      \item If $M \rightarrow_{\alpha} N$, then $MQ \rightarrow_{\alpha} NQ$ and $PM \rightarrow_{\alpha} PN$.
      \item If $M \rightarrow_{\alpha} N$, then $\lambda x. M \rightarrow_{\alpha} \lambda x. N$.
  \end{itemize}
  
  \vspace{0.5cm}
  \textbf{Alpha-Equivalence:} The reflexive, transitive closure of $\rightarrow_{\alpha}$ is denoted as $\equiv_{\alpha}$, meaning two terms are identical up to renaming.
\end{frame}


% Section 5: Beta-Reduction and Church–Rosser
\section{Beta-Reduction and Church–Rosser}
\begin{frame}{Beta-Reduction}
  \begin{block}{Definition}
    The relation \(\rightarrow_\beta\) called immediate $\beta$-reduction is the smallest relation satisying the property for all $\lambda$-terms \(M,N,P,Q\)
    \[
      (\lambda x.M)N \rightarrow_\beta M[x := N] \text{ where M is safe for substitution \([x := N]\).}
    \]
    \[
      \text{if } M \rightarrow_\beta N \text{ then } MQ \rightarrow_\beta NQ \text{ and } PM \rightarrow_\beta PN
    \]
    \[
      \text{if } M \rightarrow_\beta N \text{ then } \lambda x.M \rightarrow_\beta \lambda x.N
    \]
  \end{block}
  \begin{itemize}
    \item Transitive closure of $\rightarrow_\beta$ is denoted as $\rightarrow^+_\beta$.
    \item reflexive and transitive closure of $\rightarrow_\beta$ is denoted as $\rightarrow^*_\beta$.
    \item \textbf{\(\beta\)-conversion} denoted by $\xleftrightarrow{*}$ is smallest equivalence relation such that:
      \[
        \xleftrightarrow{*} = (\rightarrow_\beta \cup \rightarrow_\beta^{-1})
      \]
  \end{itemize}
  

\end{frame}

\begin{frame}{Examples of $\beta$-Reduction}
  \begin{block}{Example}
    \[
      (\lambda x.x) y \rightarrow_\beta (x)[x := y] \rightarrow_\beta y
    \]
    \[
      (\lambda xy.y)uv = (\lambda x.(\lambda y.y)u)v \rightarrow_\beta ((\lambda y.y)[x:=u])v = (\lambda y.y)v \rightarrow_\beta v 
    \]
  \end{block}
  \begin{block}{example}
    Let \(\omega = \lambda x.(xx)\) then
    \[
      \Omega = \omega \omega = (\lambda x.(xx))(\lambda x.(xx)) \rightarrow_\beta (\lambda x.(xx))[x := \lambda x.(xx)] = \omega\omega =\Omega
    \]
    This example shows that \(\beta\)-reduction may be infinite. This is what gives lambda calculus its power.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Example of $\beta$-Reduction}

  \begin{block}{Example}
    \(\beta\)-reduction can have growing terms also:
    \[
      (\lambda x.xxx)(\lambda x.xxx) \rightarrow_\beta (\lambda x.xxx)[x := \lambda x.xxx] = (\lambda x.xxx)(\lambda x.xxx)(\lambda x.xxx)
    \]
    \[
      \rightarrow_\beta (\lambda x.xxx)(\lambda x.xxx)(\lambda x.xxx)(\lambda x.xxx)
    \]
    \[
      \rightarrow_\beta (\lambda x.xxx)(\lambda x.xxx)(\lambda x.xxx)(\lambda x.xxx)\cdots
    \]
  \end{block}
\end{frame}

\begin{frame}{Church–Rosser Theorem}
  % \begin{itemize}
  %   \item Confluence: If \( M \) reduces to both \( M_1 \) and \( M_2 \), then there exists \( M_3 \) such that both \( M_1 \) and \( M_2 \) reduce to \( M_3 \).
  %   \item Implies uniqueness (up to $\alpha$-conversion) of normal forms.
  % \end{itemize}

  \begin{theorem}
    The following properties hold for the \(\lambda\)-calculus:
    \begin{itemize}
      \item \textbf{Confluence:} If \( M \rightarrow^*_\beta N_1 \) and \( M \rightarrow^*_\beta N_2 \), then there exists \( N_3 \) such that \( N_1 \rightarrow^*_\beta N_3 \) and \( N_2 \rightarrow^*_\beta N_3 \).
      
      \item \textbf{Church-Rosser Property:}
        for any two $\lambda$-terms \(M\) and \(N\), if \(M \xleftrightarrow{*}_\beta N\), then there exists a $\lambda$-term \(P\) such that:
        \[
          M \rightarrow^{*}_\beta P \quad \text{and} \quad N \rightarrow^{*}_\beta P
        \]
    \end{itemize}
  \end{theorem}
\end{frame}

\begin{frame}{Church–Rosser Theorem: Visual Representation}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{figure}
      \includegraphics[width=\textwidth]{images/Church-Rosser.png}
      \caption{Church–Rosser Property}
    \end{figure}
    \column{0.5\textwidth}
    \begin{figure}
      \includegraphics[width=\textwidth]{images/Confluence.png}
      \caption{Confluence Property}
    \end{figure}
  \end{columns}
\end{frame}
\begin{frame}{Proof Sketch:(Part 1 $\Leftrightarrow$ Part 2)}
  \begin{block}{Proof: (1)$\leftarrow$ (2)}   
    Assume that (2) holds. Since $\xrightarrow{*}_\beta$ is contained in $\xleftrightarrow{*}_\beta$, if 
    \[
    M \xrightarrow{*}_\beta M_1 \quad \text{and} \quad M \xrightarrow{*}_\beta M_2,
    \]
    then $M_1 \xleftrightarrow{*}_\beta M_2$.

    \pause
    Since (2) holds, there exists some $\lambda$-term $M_3$ such that 
    \[
    M_1 \xrightarrow{*}_\beta M_3 \quad \text{and} \quad M_2 \xrightarrow{*}_\beta M_3,
    \]
    which is exactly statement (1).
  \end{block}
\end{frame}
\begin{frame}{Proof Sketch:(Part 1 $\Leftrightarrow$ Part 2)}
  \begin{block}{Key Observation:}   
      To prove that (1) implies (2), we use the fact:
      \[
        \xleftrightarrow{*}_\beta = (\xrightarrow{\ }_\beta \cup \xleftarrow{\ }_\beta)^*
      \]
      So, $M_1 \xleftrightarrow{*}_\beta M_2$ if and only if:
      \begin{itemize}
        \item[(a)] $M_1 = M_2$, or
        \item[(b)] There exists $M_3$ such that $M_1 \xrightarrow{\ }_\beta M_3$ and $M_3 \xleftrightarrow{*}_\beta M_2$, or
        \item[(c)] There exists $M_3$ such that $M_3 \xrightarrow{\ }_\beta M_1$ and $M_3 \xleftrightarrow{*}_\beta M_2$.
      \end{itemize}
  \end{block}
\end{frame}
\begin{frame}{Proof Sketch:(Part 1 $\Leftrightarrow$ Part 2)}
  \begin{block}{Proof: (1)$\rightarrow$ (2): Induction on Number of Steps in $M_1 \xleftrightarrow{*}_\beta M_2$}
    \textbf{Case (a):} $M_1 = M_2$ \\
    Then (2) holds trivially with $M_3 = M_1 = M_2$.

    \vspace{1em}
    \textbf{Case (b):} $M_1 \xrightarrow{\ }_\beta M_3$ and $M_3 \xleftrightarrow{*}_\beta M_2$ \\
    By induction hypothesis, $\exists M_4$ such that:
    \[
    M_3 \xrightarrow{*}_\beta M_4 \quad \text{and} \quad M_2 \xrightarrow{*}_\beta M_4
    \]
    Hence, 
    \[
    M_1 \xrightarrow{\ }_\beta M_3 \xrightarrow{*}_\beta M_4, \quad \text{so } M_1 \xrightarrow{*}_\beta M_4
    \]
    Thus, (2) is satisfied.

  \end{block}
\end{frame}
  \begin{frame}{Proof Sketch:(Part 1 $\Leftrightarrow$ Part 2)}
    \begin{block}{Proof: (1)$\rightarrow$ (2): Induction on Number of Steps in $M_1 \xleftrightarrow{*}_\beta M_2$}
    \textbf{Case (c):} $M_3 \xrightarrow{\ }_\beta M_1$ and $M_3 \xleftrightarrow{*}_\beta M_2$ \\
    By induction hypothesis, $\exists M_4$ such that:
    \[
    M_3 \xrightarrow{*}_\beta M_4 \quad \text{and} \quad M_2 \xrightarrow{*}_\beta M_4
    \]
    Since $M_3 \xrightarrow{\ }_\beta M_1$ and $M_3 \xrightarrow{*}_\beta M_4$, by (1), $\exists M_5$ such that:
    \[
    M_1 \xrightarrow{*}_\beta M_5 \quad \text{and} \quad M_4 \xrightarrow{*}_\beta M_5
    \]
    Hence, 
    \[
    M_1 \xrightarrow{*}_\beta M_5 \quad \text{and} \quad M_2 \xrightarrow{*}_\beta M_4 \xrightarrow{*}_\beta M_5
    \]
    proving (2).
  \end{block}
\end{frame}

% Section 6: Useful Combinators
\section{Some Useful Combinators}


\subsection{Bools, Conditional and Pairs}
\begin{frame}{Combinators I, K, and S}
  Let us define the following combinators:
  \begin{itemize}
    \item \( \mathbf{I} = \lambda x.x \) (identity function)
    \item \(\mathbf{T} = \textbf{K} = \lambda xy.x \) (True)

    \item \( \mathbf{F} =\mathbf{K}_* = \lambda xy.y \) (False)
    % \item \( \mathbf{S} = \lambda xyz. xz(yz) \) 
  \end{itemize}
  We can see some interesting properties of these combinators:
  \begin{itemize}
    \item \(\mathbf{I}M \rightarrow_\beta M\)
    \item \(\mathbf{K}MN \rightarrow_\beta M\)
    \item \(\mathbf{K}_*MN \rightarrow_\beta N\)
  \end{itemize}
\end{frame}




\begin{frame}{Conditional}
  Let us define the conditional operator:
  \[
    \text{if then else} = \lambda bxy. bxy
  \]
  then for all \(\lambda-terms\) we have:

  \begin{itemize}
    \item \(\text{if}\ \mathbf{T}\ M\ N \rightarrow_\beta M\)
    \item \(\text{if}\ \mathbf{F}\ M\ N \rightarrow_\beta N\)
  \end{itemize}
  Example:
  \[
  \begin{aligned}
      \text{if } \mathbf{T} \text{ then } P \text{ else } Q &= (\text{if then else}) \mathbf{T} P Q \\
      &= (\lambda bxy. bxy) \mathbf{T} P Q \\
      &\rightarrow_{\beta} ((\lambda xy. bxy)[b := \mathbf{T}]) P Q = (\lambda xy. \mathbf{T}xy) P Q \\
      &\rightarrow_{\beta} ((\lambda y. \mathbf{T}xy)[x := P]) Q = (\lambda y. \mathbf{T}Py) Q \\
      &\rightarrow_{\beta} (\mathbf{T}Py)[y := Q] = \mathbf{T} P Q \\
      &= \mathbf{K} P Q \xrightarrow{+}_{\beta} P,
  \end{aligned}
  \]
\end{frame}



\begin{frame}
  \frametitle{Boolean Operations}
  \begin{itemize}
    \item \textbf{And:} \(\text{And }b_1b_2 = \text{if }b_1 \text{ then (if }b_2 \text{ then \textbf{T} else \textbf{F}) else \textbf{F}}\)
    \item \textbf{Or:  } \(\text{Or }b_1b_2 = \text{if }b_1 \text{ then \textbf{T} else (if }b_2 \text{ then \textbf{T} else \textbf{F})}\)
    \item \textbf{Not:} \(\text{Not }b = \text{if }b \text{ then \textbf{F} else \textbf{T}}\)
  \end{itemize}
\end{frame}


\begin{frame}{Constructing Ordered Pairs}
  For any two \( \lambda \)-terms \( M \) and \( N \), consider the combinators \( \pi_1 \) and \( \pi_2 \) defined as:
  \begin{align*}
    \langle M, N \rangle &= \lambda z.\, z\, M\, N \\
    &= \lambda z.\, \text{if } z \text{ then } M \text{ else } N \\
    \pi_1 &= \lambda z.\, zK \\
    \pi_2 &= \lambda z.\, zK_*
  \end{align*}
  Then, we have the following \(\beta\)-reductions:
  \begin{align*}
    \pi_1 \langle M, N \rangle &\xrightarrow{\beta} M \\
    \pi_2 \langle M, N \rangle &\xrightarrow{\beta} N \\
    \langle M, N \rangle T &\xrightarrow{\beta} M \\
    \langle M, N \rangle F &\xrightarrow{\beta} N
  \end{align*}
\end{frame}
\begin{frame}{Proof}
  \begin{block}{Beta Reduction of \(\pi_1 \langle M, N \rangle\)}
    \[
      \pi_1 \langle M, N \rangle \xrightarrow{\beta} M
    \]
  \end{block}
  Proof:  
  We have:
  \begin{align*}
    \pi_1 \langle M, N \rangle &= (\lambda z.\, zK)(\lambda z.\, z M N) \\
    &\xrightarrow{\beta} (zK)[z := \lambda z.\, z M N] \\
    &= (\lambda z.\, z M N)K \\
    &\xrightarrow{\beta} (z M N)[z := K] \\
    &= K M N \xrightarrow{\beta} M
  \end{align*}
\end{frame}
\begin{frame}{Proof}
  \begin{block}{Beta Reduction of \(\pi_2 \langle M, N \rangle\)}
    \[
      \pi_2 \langle M, N \rangle \xrightarrow{\beta} N
    \]
  \end{block}
  Proof:  
  We have:
  \begin{align*}
    \pi_2 \langle M, N \rangle &= (\lambda z.\, zK_*)(\lambda z.\, z M N) \\
    &\xrightarrow{\beta} (zK_*)[z := \lambda z.\, z M N] \\
    &= (\lambda z.\, z M N)K_* \\
    &\xrightarrow{\beta} (z M N)[z := K_*] \\
    &= K_* M N \xrightarrow{\beta} N
  \end{align*}
\end{frame}
\begin{frame}{Proof}
  \begin{block}{Beta Reductions of Ordered Pair Application}
    \[
      \langle M, N \rangle T \xrightarrow{\beta} M, \quad \langle M, N \rangle F \xrightarrow{\beta} N
    \]
  \end{block}
  Proof:  
  We have:
  \begin{align*}
    \langle M, N \rangle T &= (\lambda z.\, z M N) T \\
    &\xrightarrow{\beta} T M N \\
    &\xrightarrow{\beta} M, \quad \text{(since } T = K \text{)} \\[10pt]
    \langle M, N \rangle F &= (\lambda z.\, z M N) F \\
    &\xrightarrow{\beta} F M N \\
    &\xrightarrow{\beta} N, \quad \text{(since } F = K_* \text{)}
  \end{align*}
\end{frame}

% Section 7: Representing Natural Numbers
\section{Representing Natural Numbers}
\begin{frame}{Church Numerals}
  \begin{block}{Definition}
    Church Numerals \(\mathbf{c_0,c_1,c_2,\dots}\) are defined by: 
    \[
      \mathbf{c_n} = \lambda fx. f^n x
    \] 
  \end{block}
  Observe:
  \begin{itemize}
    \item \(\mathbf{c_0} = \lambda fx. x = \mathbf{K_*}\)
    \item \(\mathbf{c_1} = \lambda fx. f x\)
    \item \(\mathbf{c_n} Fz = (\mathbf{c_n}F)z = ((\lambda fx.f^n(x))F)z \rightarrow_\beta^+ F^n(z)\)
  \end{itemize}
\end{frame}

\begin{frame}{Iteration}

  \begin{definition}
    The \textbf{Iteration} combinator \textbf{Iter} is defined as:
    \[
      \text{Iter} = \lambda n f x. n f x
    \]
  \end{definition}
  Notice that:
  \begin{itemize}
    \item Iter combinator is same as the if then else combinator. This means that if we pass a boolean to the Iter combinator then it will behave like the if then else combinator.
    \item \(\text{\textbf{Iter }} \mathbf{c_n} f x = \lambda fx. f^n x\) \\
    proof:
    \[
      \text{\textbf{Iter} } \mathbf{c_n} f x \rightarrow \mathbf{c_n} f x = (\lambda fx.f^n x)fx = f^n x
    \]

  \end{itemize}
\end{frame}

\begin{frame}{Arithmetic Operations}
  \begin{itemize}
    \item \textbf{Successor:}  
      \[
      \text{\textbf{Succ}}_\mathbf{c} = \lambda nfx. f(nfx)
      \]
      
    \item \textbf{IsZero:}  
      \[
      \text{\textbf{IsZero}}_\mathbf{c} = \lambda x. x (\mathbf{K} \mathbf{F}) \mathbf{T}
      \]
      
    \item \textbf{Addition:}  
      \[
      \text{\textbf{Add}} = \lambda mn. \text{\textbf{Iter} } m \text{ \textbf{Succ}}_\mathbf{c} \ n
      \]
      
    \item \textbf{Multiplication:}  
      \[
      \text{\textbf{Mult}} = \lambda mn. \text{\textbf{Iter} } m \text{ \textbf{Add}}_\mathbf{c} \ n
      \]
      
    \item \textbf{Exponentiation:}  
      \[
      \text{\textbf{Exp}} = \lambda mn. \text{\textbf{Iter} } n \text{ \textbf{Mult}}_\mathbf{c} \ m
      \]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Arithmetic Operations}

  \begin{block}{Example: Zero Check}
    \[
    \text{\textbf{IsZero}}_\text{\textbf{c}} = \lambda n. n (\lambda x. \mathbf{F}) \mathbf{T}
    \]
    \textbf{Proof:}
    \begin{itemize}
      \item For \(n = \mathbf{c_0}\):
      \[
      \text{\textbf{IsZero}}_\text{\textbf{c}} \mathbf{c_0} = (\lambda n. n (\lambda x. \mathbf{F}) \mathbf{T}) \mathbf{c_0}
      \]
      Substituting \(\mathbf{c_0} = \lambda fx. x\):
      \[
      \rightarrow_\beta \mathbf{c_0} (\lambda x. \mathbf{F}) \mathbf{T} = (\lambda fx. x) (\lambda x. \mathbf{F}) \mathbf{T}
      \]
      \[
      \rightarrow_\beta (\lambda x.x) \mathbf{T} = \mathbf{T}
      \]
      Hence, \(\text{\textbf{IsZero}}_\text{\textbf{c}} \mathbf{c_0} \rightarrow_\beta \mathbf{T}\).

    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Arithmetic Operations}

  \begin{block}{Evaluating \(\text{IsZero}_c \mathbf{c_1}\)}
    \begin{itemize}
      \item For \( n = \mathbf{c_1} \), we have:
      \[
      \text{IsZero}_c \mathbf{c_1} = (\lambda n. n (\lambda x. \mathbf{F}) \mathbf{T}) \mathbf{c_1}
      \]
      \item Substituting \( \mathbf{c_1} = \lambda fx. f x \):
      \[
      \rightarrow_\beta \mathbf{c_1} (\lambda x. \mathbf{F}) \mathbf{T}
      \]
      \[
      = (\lambda fx. f x) (\lambda x. \mathbf{F}) \mathbf{T}
      \]
      \item Beta reduction:
      \[
      \rightarrow_\beta (\lambda x. \mathbf{F}) \mathbf{T}
      \]
      \[
      \rightarrow_\beta \mathbf{F}
      \]
      \item Hence, we conclude:
      \[
      \text{IsZero}_c \mathbf{c_1} \rightarrow_\beta \mathbf{F}
      \]
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Arithmetic Operations}
  \begin{block}{Example: Successor}
    \[
    \text{\textbf{Succ}}_\mathbf{c} = \lambda nfx. f(nfx)
    \]
    \textbf{Proof:}
    \begin{itemize}
      \item For \(n = \mathbf{c_k}\):
      \[
      \text{\textbf{Succ}}_\mathbf{c} \mathbf{c_k} = (\lambda nfx. f(nfx)) \mathbf{c_k}
      \]
      Substituting \(n = \mathbf{c_k} = \lambda fx. f^k(x)\):
      \[
      \rightarrow_\beta^* (\lambda fx. f(\mathbf{c_k}fx)) = (\lambda fx. f(\lambda fx.f^k(x))fx)
      \]
      \[
      \rightarrow_\beta^* (\lambda fx. f(f^k(x))) = (\lambda fx.f^{k+1}(x)) = \mathbf{c_{k+1}}
      \]
    \end{itemize}
    
  \end{block}

  

\end{frame}

\begin{frame}{The Predecessor Function}
  \begin{align*}
    \text{Pred}(0) &= 0, \\
    \text{Pred}(n + 1) &= n.
  \end{align*}
  \begin{itemize}
    \item More challenging to define.
    \item Kleene\'s solution in his famous 1936 paper, uses pairs:
    \[
      \text{Pred}_K = \lambda n. \pi_2 (\text{Iter} \ n \ \lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle \ \langle c_0, c_0 \rangle).
    \]
  \end{itemize}
\end{frame}
\begin{frame}{The Predecessor Function}
  \textbf{Why Kleene's Predecessor Function Works?}\\
  We have:
  \begin{align*}
    (\lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle)^0 \langle c_0, c_0 \rangle &\xrightarrow{\beta} \langle c_0, c_0 \rangle.
  \end{align*}
\begin{block}{We claim that:}
  \begin{align*}
    (\lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle)^{n+1} \langle c_0, c_0 \rangle &\xrightarrow{\beta} \langle c_{n+1}, c_n \rangle.
  \end{align*}
\end{block}
\end{frame}
\begin{frame}{The Predecessor Function}
    \begin{align*}Claim:
      (\lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle)^{n+1} \langle c_0, c_0 \rangle &\xrightarrow{\beta} \langle c_{n+1}, c_n \rangle.
    \end{align*}
  Proof by Induction on n:\\
  \textbf{Base Case:} \(n = 0\)\\
  \begin{align*}
    (\lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle) \langle c_0, c_0 \rangle &\xrightarrow{\beta} \langle \text{Succ}(\pi_1 \langle c_0, c_0 \rangle), \pi_1 \langle c_0, c_0 \rangle \rangle \\
    &\xrightarrow{\beta} \langle \text{Succ}(c_0), c_0 \rangle \\
    &\xrightarrow{\beta} \langle c_1, c_0 \rangle.
  \end{align*}
\end{frame}

\begin{frame}{The Predecessor Function}
  \begin{align*}Claim:
    (\lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle)^{n+1} \langle c_0, c_0 \rangle &\xrightarrow{\beta} \langle c_{n+1}, c_n \rangle.
  \end{align*}
  \textbf{Inductive Step:} Assume true for \(n\), show for \(n+1\)\\
  \[
  \begin{aligned}
    &(\lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle)^{n+2} \langle c_0, c_0 \rangle \\
    &= (\lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle)((\lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle)^{n+1} \langle c_0, c_0 \rangle) \\
    &\xrightarrow{\beta} (\lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle) \langle c_{n+1}, c_n \rangle \\
    &\xrightarrow{\beta} \langle c_{n+2}, c_{n+1} \rangle.
  \end{aligned}
  \]
\end{frame}
\begin{frame}{Predecessor Functions}
  \begin{itemize}
    \item \textbf{Kleene's predecessor function:}
      \[
      \text{Pred}_K = \lambda n. \pi_2 (\text{Iter} \ n \ \lambda z. \langle \text{Succ}(\pi_1 z), \pi_1 z \rangle \ \langle c_0, c_0 \rangle).
      \]
  
    \item \textbf{Alternative definition of predecessor:}
      \[
      \text{Pred}_c = \lambda x y z.\ x(\lambda p q.\ q(p y)) (K z) I.
      \]
  \end{itemize}
\end{frame}


% Section 8: Fixed-Point Combinators and Recursion
\section{Fixed-Point Combinators and Recursion}
\begin{frame}{The Factorial: n!}
  \begin{itemize}
    \item $fact(0)=1$
    \item $fact(n)=n*fact(n-1)$
  \end{itemize}
  \begin{block}{Factorial Function}
    \texttt{
      int fact(int n) \{ \newline
      \hspace*{1em}if (n == 0) return 1; \newline
      \hspace*{1em}else return n * fact(n - 1); \newline
      \}
    }
  \end{block}
  \begin{block}{}
    \ fact=($\lambda$ n. if (IsZero n) then 1 else Mult N (fact (Pred n))) \\
  \end{block}
\end{frame}
\begin{frame}{Fixed-Point Combinators}
  \begin{itemize}
  \item Fixed-point combinators allow us to define recursive functions in the lambda calculus, giving us a way to express recursion without explicit self-reference.
  We want a ability to be able to do this:
  \[
    func(x):= \text{If [base condition] then base else func(y)}
  \]
  \item Our issue is that functions in lambda calculus are not named hence we cannot have the function be named and refrence called itself.\\
  \item \textbf{Fixed-point combinators} solves the problem by taking the function as an argument and returning a fixed point of that function.
  \end{itemize}
  \begin{block}{Turing $\Theta$-Combinator}
    \[
      \mathbf{\Theta} := \left( \lambda xy.y\left( xxy \right)\right) \left( \lambda xy.y\left( xxy \right) \right)
    \]
  \end{block}

\end{frame}

% \begin{frame}
%   \frametitle{Y-Combinator}
%   \begin{lemma}
    
%     For any \(\lambda\)-term \(F\), we have:
%     \vspace{-0.5em}
%     \[
%       F(\mathbf{Y} F) \xleftrightarrow{*}_\beta Y F
%     \]
%     \textbf{Proof:}
%     Let W = \(\lambda x. F (x x)\)\\
%     Then we have:
%     \[
%       \begin{aligned}
%       \mathbf{Y} F &= (\lambda f. (\lambda x. f (x x)) (\lambda x. f (x x))) F
%       \rightarrow_\beta (\lambda x. F (x x)) (\lambda x. F (x x)) \\
%       &= (\lambda x. F (x x))W       \rightarrow_\beta F(W W) \\
%     \end{aligned}
%     \]
%     and:
%     \[
%       \begin{aligned}
%       F(\mathbf{Y} F) = F((\lambda f. (\lambda x. f (x x)) (\lambda x. f (x x))) F) &\rightarrow_\beta F((\lambda x. F (x x)) (\lambda x. F (x x))) \\
%       &= F(WW)
%       \end{aligned}
%     \]
%     and Therfore: \(F(\mathbf{Y}F) \xleftrightarrow{*}_\beta \mathbf{Y}F\)
    
%   \end{lemma}
% \end{frame}

\begin{frame}{Turing's $\Theta$-Combinator}
  \begin{itemize}
  % \item An issue with Y-combinator is the it does allow one step-self reference ie. \(F(\mathbf{Y}F) \xleftrightarrow{*}_\beta \mathbf{Y}F\) and not \( \mathbf{Y}F \xrightarrow[]{\text{+}}_\beta F(\mathbf{Y}F)\).
  \item We define \textit{Turing $\mathbf{\Theta}$-combinator} as:
  \[ \mathbf{\Theta} := \left( \lambda xy.y\left( xxy \right)\right) \left( \lambda xy.y\left( xxy \right) \right) \]
  \item Now, for any $\lambda$-term \(F\), we have:
    \[
      \mathbf{\Theta}F \xrightarrow[]{\text{+}}_\beta F(\mathbf{\Theta}F)
    \]
    % \textit{Proof:} Writing \( A = (\lambda xy.y(xxy))\). Thus,
    % \( \mathbf{\Theta} = AA \). Now,
    % \begin{align*}
    %   \mathbf{\Theta}F &= AA F = ((\lambda xy.y(xxy))A)F\\
    %   &\xrightarrow[]{}_\beta (\lambda y.y(AAy))F \\
    %   &\xrightarrow[]{}_\beta F(AAF)\\
    %   &= F(\mathbf{\Theta}F)\\
    %   & \hspace{5cm} \blacksquare
    % \end{align*}
  \end{itemize}
  \begin{block}{Proof}
    Writing \( A = (\lambda xy.y(xxy)) \). Thus,  
    \( \mathbf{\Theta} = AA \). Now,
    \begin{align*}
      \mathbf{\Theta}F &= AA F = ((\lambda xy.y(xxy))A)F\\
      &\xrightarrow[]{}_\beta (\lambda y.y(AAy))F \\
      &\xrightarrow[]{}_\beta F(AAF)\\
      &= F(\mathbf{\Theta}F)
    \end{align*}
  \end{block}
  \small The conbinator takes our function as outputs equivalent of \(\lambda u.(F(F(\dots(F(u))))\)
    
\end{frame}

\begin{frame}{Defining Recursive Functions}
  \begin{itemize}
    \item To define a recursive function \(G\) such that
      \[
      G X \rightarrow_\beta M(X, G)
      \]
    \item Let \(F = \lambda g x. M(x, g)\) and define \(G = \Theta F\).
  \end{itemize}
  \begin{block}{Example: Factorial Function}
    Define:
    \[
      F = \lambda g\ n.\, \text{if}\ (\text{IsZero}\ n)\ \text{then}\ c_1\ \text{else}\ \text{Mult}\ n\ \bigl(g\ (\text{Pred}\ n)\bigr)
      \]
    Then \(G = \Theta F\) represents the factorial function.
  \end{block}
\end{frame}


\begin{frame}{Proof: Factorial Function}
  \begin{itemize}
    \item Let's prove that \(G\) represents the factorial function by induction.
    \item Base case: \(n = 0\)
      \[
      G c_0 \rightarrow_\beta F (G) c_0 \rightarrow_\beta c_1
      \]
    \item Inductive step: Assume true for \(n\), show for \(n+1\)
    \item   Inductive hypothesis:
      \[
      G c_n \rightarrow_\beta c_{n!}
      \]
      \[
      G c_{n+1} \rightarrow_\beta F (G) c_{n+1} \rightarrow_\beta \text{Mult} (c_{n+1}) (G c_n) \rightarrow_\beta \text{Mult} (c_{n+1}) (c_{n!})
      \]
      \[
      G c_{n+1} \rightarrow_\beta F (G) c_{n+1} \rightarrow_\beta \text{Mult} (c_{n+1}) (G c_n) \rightarrow_\beta \text{Mult} (c_{n+1}) (c_{n!})
      \]
    \item Hence, \(G\) computes the factorial function.
  \end{itemize}
  \vspace{-0.5em} % Adjust spacing to avoid overfull box
\end{frame}
 

% Section 9: Lambda-Definability of Computable Functions
\section{Lambda-Definability of Computable Functions}

\subsection{Computable Functions}
\begin{frame}{Computable Functions}
  First we need to define what we mean by computable functions.\\
  We will use definition given by (a la Herbrand-Kleene-Gödel) of recursive functions.

  \begin{itemize}
    \item \textbf{Base Functions:} Zero, successor, and projection functions.
    \item \textbf{Closure under Composition:} If \(f\) and \(g\) are computable, then \(h(x_1,\dots,x_n) = f(g(x_1,\dots,x_n),\dots)\) is computable.
    \item \textbf{Primitive Recursion:} If \(f\) is computable, then the function defined by:
      \[
      h(0,x_1,\dots,x_n) = f(x_1,\dots,x_n)
      \]
      \[
      h(n+1,x_1,\dots,x_n) = g(n,h(n,x_1,\dots,x_n))
      \]
      is also computable.
    \item \textbf{Minimization:} If \(f\) is computable, then the function defined by:
      \[
      h(x_1,\dots,x_n) = \min\{n : f(n,x_1,\dots,x_n) = 0\}
      \]
      is also computable.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Base Functions}
  \begin{block}{Base Functions}
    Base functions Z, S, and \(P_i^n\) are defined as:
    \begin{enumerate}
      \item Zero function:
        \[
          Z(n) = 0 \forall n \in \mathbb{N}
        \]
      \item Successor function:
        \[
          S(n) = n + 1 \forall n \in \mathbb{N}
        \]
      \item Projection function: For every \(n\geq1\) and every i with \(1\leq i \leq n\):
        \[
          P_i^n(x_1,\dots,x_n) = x_i
        \]
    \end{enumerate}
    
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Composition}
  \begin{definition}
    Given function \( g: \mathbb{N}^m \to \mathbb{N} \) (\( m \geq 1 \)) and any \( m \) functions \( h_i: \mathbb{N}^n \to \mathbb{N} \) (\( n \geq 1 \)), the \textbf{composition} of \( g \) and \( h_1, \dots, h_m \), denoted \( g \circ (h_1, \dots, h_m) \), is the function \( f: \mathbb{N}^n \to \mathbb{N} \) given by:
    \[
      f(x_1, \dots, x_n) = g(h_1(x_1, \dots, x_n), \dots, h_m(x_1, \dots, x_n)),
    \]
    where \( x_1, \dots, x_n \in \mathbb{N} \).
  \end{definition}
\end{frame}

\begin{frame}
  \frametitle{Primitive Recursion}

  \begin{definition}
    Given any functions \( g: \mathbb{N}^m \to \mathbb{N} \) and \( h: \mathbb{N}^{m+2} \to \mathbb{N} \) (\( m \geq 1 \)), the function \( f: \mathbb{N}^{m+1} \to \mathbb{N} \) is defined by \textbf{primitive recursion} as:
    \[
      f(0, x_1, \dots, x_m) = g(x_1, \dots, x_m)
    \]
    \[
      f(n+1, x_1, \dots, x_m) = h(f(n, x_1, \dots, x_m), n, x_1, \dots, x_m)
    \]
    for all \( n, x_1, \dots, x_m \in \mathbb{N} \). \\
    If \( m = 0 \), then \( g \) is some fixed natural number, and we have:
    \[
      f(0) = g, \quad f(n+1) = h(f(n), n).
    \]
  \end{definition}

  % \begin{itemize}
  %   \item If \( g \) and \( h \) are total functions, then so is \( f \).
  % \end{itemize}
\end{frame}

% \begin{frame}
%   \frametitle{Primitive Recursion: Argument Permutation}

%   \begin{itemize}
%     \item The second clause of primitive recursion is:
%     \[
%       f(n+1, x_1, \dots, x_m) = h(f(n, x_1, \dots, x_m), n, x_1, \dots, x_m) \quad \text{(*1)}
%     \]
%     but in an earlier definition, it was:
%     \[
%       f(n+1, x_1, \dots, x_m) = h(n, f(n, x_1, \dots, x_m), x_1, \dots, x_m) \quad \text{(*2)}
%     \]
%     with the first two arguments of \( h \) permuted.
%   \end{itemize}

%   \begin{block}{Argument Permutation}
%     \[
%       h \circ (P^{m+2}_2, P^{m+2}_1, P^{m+2}_3, \dots, P^{m+2}_{m+2})(n, f(n, x_1, \dots, x_m), x_1, \dots, x_m)
%     \]
%     \[
%       = h(f(n, x_1, \dots, x_m), n, x_1, \dots, x_m)
%     \]
%     and
%     \[
%       h \circ (P^{m+2}_2, P^{m+2}_1, P^{m+2}_3, \dots, P^{m+2}_{m+2})(f(n, x_1, \dots, x_m), n, x_1, \dots, x_m)
%     \]
%     \[
%       = h(n, f(n, x_1, \dots, x_m), x_1, \dots, x_m).
%     \]
%   \end{block}
% \end{frame}

\begin{frame}
  \frametitle{Minimization}

  \begin{definition}
    Given any function \( g: \mathbb{N}^{m+1} \to \mathbb{N} \) (\( m \geq 0 \)), the function \( f: \mathbb{N}^m \to \mathbb{N} \) is defined as follows:

    \[
      f(x_1, \dots, x_m) = \text{the least } n \in \mathbb{N} \text{ such that } g(n, x_1, \dots, x_m) = 0,
    \]
    and is \textbf{undefined} if there is no such \( n \) satisfying this condition.
  \end{definition}

  \begin{block}{Notation}
    We say \( f \) is \textbf{defined by minimization} from \( g \), and write:
    \[
      f(x_1, \dots, x_m) = \mu x \, [g(x, x_1, \dots, x_m) = 0].
    \]
    For short, we write \( f = \mu g \).
  \end{block}

  % \begin{itemize}
  %   \item Even if \( g \) is a total function, \( f \) may be undefined for some (or all) of its inputs.
  % \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Computable Functions}

  \begin{definition}
    \textbf{Definition 3.17} (Herbrand–Gödel–Kleene). The set of \textit{partial computable} (or \textit{partial recursive}) functions is the smallest set of partial functions (defined on \( \mathbb{N}^n \) for some \( n \geq 1 \)) which contains the base functions and is closed under:
    \begin{enumerate}
      \item Composition.
      \item Primitive recursion.
      \item Minimization.
    \end{enumerate}
  \end{definition}

  \begin{block}{Computable (Recursive) Functions}
    The set of \textit{computable} (or \textit{recursive}) functions is the subset of partial computable functions that are \textbf{total functions} (i.e., defined for all inputs).
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Computable Functions}
  \begin{block}{Kleene Normal Form}
    It can be proven every partial computable function \( f: \mathbb{N}^m \to \mathbb{N} \) is computable as:
    \[
      f = g \circ \mu h,
    \]
    for some \textit{primitive recursive functions} \( g: \mathbb{N} \to \mathbb{N} \) and \( h: \mathbb{N}^{m+1} \to \mathbb{N} \).
  \end{block}

  \begin{itemize}
    \item The significance of this result is that \( f \) is built from \textbf{total functions} using composition and primitive recursion, with only a single minimization at the end.
    \item Before stating the main theorem, we need to define what it means for a numerical function to be definable in \( \lambda \)-calculus.
  \end{itemize}

\end{frame}

\subsection{Lambda-Definability}
\begin{frame}{Lambda-Definability}
  % \begin{itemize}
  %   \item A function \(f\) is \(\lambda\)-definable if there exists a closed \(\lambda\)-term \(F\) such that:
  %     \begin{enumerate}
  %       \item \(F\,c_{m_1}\cdots c_{m_n}\) has a normal form if and only if \(f(m_1,\dots,m_n)\) is defined.
  %       \item When defined, \(F\,c_{m_1}\cdots c_{m_n} \rightarrow_\beta c_{f(m_1,\dots,m_n)}\).
  %     \end{enumerate}
  % \end{itemize}
  \begin{definition}
    A function \(f: \mathbb{N}^n \to \mathbb{N}\) is said to be \textbf{\(\lambda\)-definable} if there exists a closed \(\lambda\)-term \(F\) such that:
    \begin{enumerate}
      \item \(F\,c_{m_1}\cdots c_{m_n}\) has a normal form if and only if \(f(m_1,\dots,m_n)\) is defined.
      \item If defined, \(F\,c_{m_1}\cdots c_{m_n} \rightarrow_\beta c_{f(m_1,\dots,m_n)}\).
    \end{enumerate}
  \end{definition}
\end{frame}

\begin{frame}{Theorem Overview}

  \begin{block}{Theorem}
    If a (total) function $f: \mathbb{N}^n \to \mathbb{N}$ is computable, then it is $\lambda$-definable.
  
    If a (partial) function $f: \mathbb{N}^n \to \mathbb{N}$ is partial computable, then it is $\lambda$-definable.  
  \end{block}


  Since the definition of computable functions is also same for turing machince the above theorem also tells us that the lambda calculus has same power as a Turing machine.
  \begin{itemize}
    \item Every total computable function is \(\lambda\)-definable.
    \item Every partial computable function is also \(\lambda\)-definable.
    \item This establishes the equivalence between the lambda-calculus and Turing machines.
  \end{itemize}
\end{frame}


\begin{frame}{Proof Outline}
  \textbf{Step 1: Base Case}
  \begin{itemize}
      \item The base functions are $\lambda$-definable.
      \item $\mathbf{Z_c}$ computes $Z$, and $\mathbf{Succ_c}$ computes $S$.
      \item The function $U_i^n$ given by $U_i^n = \lambda x_1 \dots x_n. x_i$ computes $P_i^n$.
  \end{itemize}
\end{frame}

\begin{frame}{Step 2: Closure under Composition}
  If $g$ is $\lambda$-defined by $G$ and $h_1, \dots, h_m$ are $\lambda$-defined by $H_1, \dots, H_m$, then $g \circ (h_1, \dots, h_m)$ is $\lambda$-defined by:
  \[
  F = \lambda x_1 \dots x_n. G(H_1 x_1 \dots x_n) \dots (H_m x_1 \dots x_n)
  \]
\end{frame}

\begin{frame}{Step 3: Closure under Primitive Recursion}
  \begin{itemize}
      \item If $f$ is defined by primitive recursion from $g$ and $h$, and $G$ and $H$ $\lambda$-define them, then:
      \small \[  
      F = \lambda n x_1 \dots x_m. \pi_1 (\text{Iter} n \lambda z. \langle H \pi_1 z \pi_2 z x_1 \dots x_m, \mathbf{Succ_c}(\pi_2 z) \rangle \langle G x_1 \dots x_m, c_0 \rangle)
      \]
      \item This ensures $F$ $\lambda$-defines $f$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Step 3: Closure under Primitive Recursion}
  \textbf{Proof:} We will prove by induction
  \small\[
     (\lambda z. \langle H \pi_1 z \pi_2 z \mathbf{c_{n_1}} \dots \mathbf{c_{n_m}}, \mathbf{Succ_c}(\pi_2 z) \rangle)^n \langle G \mathbf{c_{n_1}} \dots \mathbf{c_{n_m}}, \mathbf{c_0} \rangle \rightarrow_\beta \langle \mathbf{c}_{f(n,n_1,\dots,n_m)}, \mathbf{c_{n}} \rangle
  \]
  Base case: \(n = 0\)\\
  \[
  \left( \lambda z. \langle H \, \pi_1 z \, \pi_2 z \, \mathbf{c}_{n_1} \cdots \mathbf{c}_{n_m}, \mathbf{Succ_c}(\pi_2 z) \rangle \right)^0 \langle G \mathbf{c}_{n_1} \cdots \mathbf{c}_{n_m}, \mathbf{c}_0 \rangle
  \]

  \[
  \overset{+}{\longrightarrow}_{\beta} \langle G \mathbf{c}_{n_1} \cdots \mathbf{c}_{n_m}, \mathbf{c}_0 \rangle = \langle \mathbf{c}_{g(n_1, \dots, n_m)}, \mathbf{c}_0 \rangle = \langle \mathbf{c}_{f(0, n_1, \dots, n_m)}, \mathbf{c}_0 \rangle.
  \]
\end{frame}

\begin{frame}
  \frametitle{Step 3: Closure under Primitive Recursion}

  \begin{align*}
    &(\lambda z. \langle H \pi_1 z \pi_2 z \mathbf{c_{n_1}} \cdots \mathbf{c_{n_m}}, \mathbf{Succ_c} (\pi_2 z)) \rangle^{n+1} \langle G \mathbf{c_{n_1}} \cdots \mathbf{c_{n_m}}, \mathbf{c_0} \rangle \\
    &= (\lambda z. \langle H \pi_1 z \pi_2 z \mathbf{c_{n_1}} \cdots \mathbf{c_{n_m}}, \mathbf{Succ_c} (\pi_2 z)) \rangle \\
    &\quad (\lambda z. \langle H \pi_1 z \pi_2 z \mathbf{c_{n_1}} \cdots \mathbf{c_{n_m}}, \mathbf{Succ_c} (\pi_2 z)) \rangle^n \langle G \mathbf{c_{n_1}} \cdots \mathbf{c_{n_m}}, \mathbf{c_0} \rangle) \\
    &\xrightarrow{+}_\beta (\lambda z. \langle H \pi_1 z \pi_2 z \mathbf{c_{n_1}} \cdots \mathbf{c_{n_m}}, \mathbf{Succ_c} (\pi_2 z)) \rangle \langle \mathbf{c}_{f(n, n_1, ..., n_m)}, \mathbf{c_n} \rangle \\
    &\xrightarrow{+}_\beta \langle H \mathbf{c}_{f(n, n_1, ..., n_m)} \mathbf{c_n} \mathbf{c_{n_1}} \cdots \mathbf{c_{n_m}}, \mathbf{Succ_c} \mathbf{c_n} \rangle \\
    &\xrightarrow{+}_\beta \langle \mathbf{c}_{h(f(n, n_1, ..., n_m), n, n_1, ..., n_m}, \mathbf{c_{n+1}} \rangle = \langle \mathbf{c}_{f(n+1, n_1, ..., n_m)}, \mathbf{c_{n+1}} \rangle.
    \end{align*}

\end{frame}

\begin{frame}{Step 4: Closure under Minimization}
  Suppose $f$ is total and defined by minimization from $g$, where $g$ is $\lambda$-defined by $G$. Define:
  \[
  J = \lambda f x x_1 \dots x_m. \text{if } \text{\textbf{IsZero}}_\mathbf{c} G x x_1 \dots x_m \text{ then } x \text{ else } f(\mathbf{Succ_c} x) x_1 \dots x_m
  \]
  \[
  F = \Theta J
  \]
  Clearly:
  \[
    F \mathbf{c}_n \mathbf{c}_{n_1} \cdots \mathbf{c}_{n_m} \xrightarrow{+}_\beta 
    \begin{cases} 
    \mathbf{c}_n & \text{if } g(n, n_1, \ldots, n_m) = 0 \\ 
    F \mathbf{c}_{n+1} \mathbf{c}_{n_1} \cdots \mathbf{c}_{n_m} & \text{otherwise}
    \end{cases}
  \]
  \begin{itemize}
      \item This ensures that $F$ $\lambda$-defines $f$.
      \item Since $F$ is total, some least $n$ will be found.
  \end{itemize}
\end{frame}

\begin{frame}{Partial Computable Functions}
  To prove the result for partial computable functions, we use the Kleene normal form:
  \[
  f = g \circ \mu h
  \]
  where $g$ and $h$ are primitive recursive.
  \begin{itemize}
      \item Our previous proof ensures $g$ and $h$ are $\lambda$-definable.
      \item Minimization may fail, but since $g$ is total, it remains well-defined.
      \item A rigorous proof is available in Hindley and Seldin (Chapter 4, Theorem 4.18).
  \end{itemize}
\end{frame}

\begin{frame}{Conclusion}
  \begin{itemize}
      \item With some work, it is possible to show that lists can be represented in the \(\lambda\)-calculus:
      \begin{itemize}
        \item We have to use the Pair combinators to store pair within a pair to form a linked list.
      \end{itemize}
      \item Since the tape of turing machine can be represented as a list, we can simulate a Turing machine using the \(\lambda\)-calculus. (Tedious Task)
      \item The Construction of turing machine in \(\lambda\)-calculus mimics the proof that Turing machince computes a computable function.
  \end{itemize}
\end{frame}

\begin{frame}{Remark}
  \begin{itemize}
      \item $\lambda$-calculus has the same power as Turing machines.
      \item This leads to undecidability results similar to the halting problem and Rice’s theorem.
  \end{itemize}
  \textbf{Scott-Curry Theorem:} An analog of Rice’s theorem follows as a corollary.
\end{frame}


% Section 10: Summary and Conclusions
\section{Summary and Conclusions}
\begin{frame}{Summary}
  \begin{itemize}
    \item The lambda-calculus provides a minimalistic foundation for computation.
    \item Its syntax is based on variables, abstraction, and application.
    \item Substitution and $\alpha$-conversion manage variable binding.
    \item $\beta$-reduction drives computation.
    \item Combinators, Church numerals, and fixed-point combinators illustrate its power.
    \item Every computable function is $\lambda$-definable.
  \end{itemize}
\end{frame}

\begin{frame}{Implications}
  \begin{itemize}
    \item Equivalence to Turing machines shows universal computation.
    \item Fundamental for the design of functional programming languages.
    \item Offers insights into recursion and fixed-point theory.
  \end{itemize}
\end{frame}

\begin{frame}{Further Directions}
  \begin{itemize}
    \item Study evaluation strategies (call-by-name, call-by-value).
    \item Explore type systems: Simply typed lambda-calculus.
  \end{itemize}
\end{frame}


\begin{frame}{Acknowledgements}
  These slides were made for seminar presentation for \textbf{Automata Theory and Computability} [UMC205] 2025 course at \textbf{Indian Institute of Science, Bengaluru} under the guidance of \textbf{Prof. Deepak D'Souza}. \\
  We have refrenced:
  \begin{itemize}
    \item Proofs, Computability, Undecidability, Complexity, And the Lambda Calculus An Introduction [Jean Gallier and Jocelyn Quaintance]
  \end{itemize}
  Team Members:
  \begin{itemize}
    \item \textbf{Gavish Bansal:} gavishbansal@iisc.ac.in
    \item \textbf{Sehaj Ganjoo:} sehajganjoo@iisc.ac.in
    \item \textbf{Pratham Gupta:} prathamgupta@iisc.ac.in
    \item \textbf{Krishna Agarwal:} krishnaagarw@iisc.ac.in
  \end{itemize}
\end{frame}

\begin{frame}{Questions?}
  \begin{center}
    \Large Thank you for your attention!\\[1em]
    Any Questions?
  \end{center}
\end{frame}

% Additional slides can be added to reach the 50-60 slide target by expanding on proofs, examples, and further details.

\end{document}